{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pos-Tagging.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "biEcrt1bnqgU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import KFold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-DGDQadb859",
        "colab_type": "text"
      },
      "source": [
        "## Load data and create transition, emission and start probabilities matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMJyK7J0HlZQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def insert_dict(transictions_dict, feature, adjacent_feature):\n",
        "    \"\"\"\n",
        "    Count transition to each feature\n",
        "    \"\"\"\n",
        "    if feature in transictions_dict:\n",
        "        if adjacent_feature in transictions_dict[feature]:\n",
        "            transictions_dict[feature][adjacent_feature] += 1\n",
        "        else:\n",
        "            transictions_dict[feature][adjacent_feature] = 1\n",
        "    else:\n",
        "        transictions_dict[feature] = {adjacent_feature:1}\n",
        "    \n",
        "    return transictions_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etU7tz-Ub1o7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_transition_and_emission_matrix(train_sentences):\n",
        "    \"\"\"\n",
        "    Generate transition and emission probabilities\n",
        "    matrix using a list of sentences with features\n",
        "    \"\"\"\n",
        "    token_dict = {}\n",
        "    feature_list = []\n",
        "    transitions_count = {}\n",
        "    hapax_words = {}\n",
        "    for line in train_sentences:\n",
        "        raw_tokens_list = line.split()\n",
        "        list_size = len(raw_tokens_list)\n",
        "        sentence_list = []\n",
        "        # Counting features to create emission probabilities\n",
        "        for index, raw_token in enumerate(raw_tokens_list):\n",
        "            splitted_token = raw_token.split('_')\n",
        "            token = splitted_token[0]\n",
        "            feature = splitted_token[1]\n",
        "            feature_list.append(feature)\n",
        "\n",
        "            # Creating sequence of features\n",
        "            if index==0:\n",
        "                sentence_list.append('start_token')\n",
        "            sentence_list.append(feature)\n",
        "            if index==(list_size-1):\n",
        "                # if it is last item\n",
        "                sentence_list.append('end_token')\n",
        "\n",
        "            # Create hapax word list with 0 if it reapeat\n",
        "            # or the tag if it doesn't\n",
        "            if token in hapax_words:\n",
        "                hapax_words[token] = 0\n",
        "            else:\n",
        "                hapax_words[token] = feature\n",
        "\n",
        "            if token in token_dict:\n",
        "                if feature in token_dict[token]:\n",
        "                    token_dict[token][feature] +=1\n",
        "                else:\n",
        "                    token_dict[token][feature] =1\n",
        "            else:\n",
        "                token_dict[token] = {feature : 1}\n",
        "        \n",
        "        # Counting transitions to each feature to calculate transition probabilities\n",
        "        for idx, feature in enumerate(sentence_list):\n",
        "            if feature=='end_token':\n",
        "                break\n",
        "            transitions_count = insert_dict(transitions_count, feature, sentence_list[idx+1])\n",
        "\n",
        "    # Sorted\n",
        "    token_dict = dict(sorted(token_dict.items()))\n",
        "    token_matrix = pd.DataFrame(token_dict.values(),index=token_dict.keys()).fillna(0)\n",
        "    # Create emission probabilities\n",
        "    emission_matrix_probabilities =  token_matrix/token_matrix.sum()\n",
        "\n",
        "    transition_matrix = pd.DataFrame(transitions_count.values(), index=transitions_count.keys()).fillna(0)\n",
        "    # Create transition probabilities\n",
        "    for index, row in transition_matrix.iterrows():\n",
        "        transition_matrix.loc[index] = transition_matrix.loc[index]/transition_matrix.loc[index].sum()\n",
        "    transition_matrix_probabilities = transition_matrix.copy()\n",
        "    transition_matrix_probabilities.drop(columns='end_token', inplace=True)\n",
        "\n",
        "    start_probabilities = transition_matrix_probabilities.loc['start_token'].sort_index().copy()\n",
        "    # To make a sorted matrix with diagonal being transition for its own feature\n",
        "    transition_matrix_probabilities = transition_matrix_probabilities.drop('start_token').sort_index().T.sort_index().T\n",
        "    emission_matrix_probabilities = emission_matrix_probabilities.T.sort_index()\n",
        "\n",
        "    # DEALING WITH UNKNOWN WORDS:\n",
        "    # Create hapax Series with unique words\n",
        "    hapax_series = pd.Series(hapax_words)\n",
        "    hapax_series = hapax_series[hapax_series!=0]\n",
        "    \n",
        "    # Count tags present on unique words\n",
        "    hapax_legomena_count = {}\n",
        "    for token in hapax_series.keys():\n",
        "        tag = hapax_series[token]\n",
        "        if tag in hapax_legomena_count:\n",
        "            hapax_legomena_count[tag] += 1\n",
        "        else:\n",
        "            hapax_legomena_count[tag] = 1\n",
        "    \n",
        "    # Crete hepax distribution\n",
        "    hapax_legomena_series = pd.Series(hapax_legomena_count)\n",
        "    hapax_legomena_distribution = hapax_legomena_series/hapax_legomena_series.sum()\n",
        "\n",
        "    \n",
        "    # Set unknown token to have this distribution\n",
        "    emission_matrix_probabilities['unknown_token'] = hapax_legomena_distribution\n",
        "    emission_matrix_probabilities['unknown_token'].fillna(0, inplace=True)\n",
        "\n",
        "    # Normalize probabilities\n",
        "    emission_matrix_probabilities = (emission_matrix_probabilities.T/emission_matrix_probabilities.T.sum()).T\n",
        "    \n",
        "    return emission_matrix_probabilities , transition_matrix_probabilities, start_probabilities"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBQ-3lOn-Kvd",
        "colab_type": "text"
      },
      "source": [
        "## Viterbi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8I_2SJGTaLM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(object):\n",
        "    '''\n",
        "    The Decoder class implements the Viterbi algorithm\n",
        "    Parameters\n",
        "    ----------\n",
        "      initialProb: np.array Tx1\n",
        "      The initial probability $P(t_i)$\n",
        "      transProb: np.array NxN\n",
        "      The transition matrix $P(t_i|t_{i-1})$\n",
        "      obsProb: np.array NxT\n",
        "      The emission matrix $P(w_i|t_i)$\n",
        "    Attributes\n",
        "    ----------\n",
        "        N : int\n",
        "        The number of states (tags in POS-Tagging)\n",
        "        initialProb:\n",
        "        A priori probability of stats ($P(t_i)$ in POST)\n",
        "        transProb:\n",
        "        Transition matrix ($P(t_i|t{i-1})$ in POST)\n",
        "        obsProb:\n",
        "        Emission matrix ($P(w_i|t_i)$ in POST)\n",
        "    '''\n",
        "\n",
        "\n",
        "    def __init__(self, initialProb, transProb, obsProb):\n",
        "        self.N = initialProb.shape[0]\n",
        "        self.initialProb = initialProb\n",
        "        self.transProb = transProb\n",
        "        self.obsProb = obsProb\n",
        "        assert self.initialProb.shape == (self.N, 1)\n",
        "        assert self.transProb.shape == (self.N, self.N)\n",
        "        assert self.obsProb.shape[0] == self.N # no control over 2nd dimension\n",
        "\n",
        "    def Obs(self, obs):\n",
        "        return self.obsProb[:, obs, None]\n",
        "\n",
        "    def Decode(self, obs):\n",
        "        '''\n",
        "        This is the Viterbi algorithm\n",
        "        Parameters\n",
        "        ----------\n",
        "        obs : list\n",
        "            DESCRIPTION.\n",
        "        Returns\n",
        "        -------\n",
        "        list\n",
        "            List of states\n",
        "        '''\n",
        "        trellis = np.zeros((self.N, len(obs)))\n",
        "        backpt = np.ones((self.N, len(obs)), 'int32') * -1\n",
        "\n",
        "        # initialization\n",
        "        trellis[:, 0] = np.squeeze(self.initialProb * self.Obs(obs[0]))\n",
        "\n",
        "        # steps\n",
        "        for t in range(1, len(obs)):\n",
        "            trellis[:, t] = (trellis[:, t-1, None].dot(self.Obs(obs[t]).T) *\n",
        "                             self.transProb).max(0)\n",
        "            backpt[:, t] = (np.tile(trellis[:, t-1, None], [1, self.N]) *\n",
        "                            self.transProb).argmax(0)\n",
        "\n",
        "        # termination\n",
        "        tokens = [trellis[:, -1].argmax()]\n",
        "        for i in range(len(obs)-1, 0, -1):\n",
        "            tokens.append(backpt[tokens[-1], i])\n",
        "\n",
        "        return tokens[::-1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIscxwyIbbJC",
        "colab_type": "text"
      },
      "source": [
        "## Measure accuracy of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsybWE3rZVVk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy_on_test(test_sentences, viterbi_obj, emission_matrix_probabilities, transition_matrix_probabilities):\n",
        "    \"\"\"\n",
        "    Check accuracy on total features and with each feature \n",
        "    individually on test setences\n",
        "    \"\"\"\n",
        "    feature_corrected_predicted = {}\n",
        "    total_accuracy = 0\n",
        "    features_total_frequency = Counter()\n",
        "    for line in tqdm(test_sentences):\n",
        "        token_list = []\n",
        "        feature_list = []\n",
        "        raw_tokens_list = line.split()\n",
        "        # Split for token and feature to check predictions\n",
        "        for raw_token in raw_tokens_list:\n",
        "            splitted_token = raw_token.split('_')\n",
        "            token = splitted_token[0]\n",
        "            token_list.append(token)\n",
        "            feature = splitted_token[1]\n",
        "            feature_list.append(feature)\n",
        "        word_positions = []\n",
        "        for token in token_list:\n",
        "            try:\n",
        "                # Get position of token\n",
        "                position = emission_matrix_probabilities.columns.get_loc(token)\n",
        "            except:\n",
        "                # if word not in the emission probabilities\n",
        "                # get unknown_token position with hapax\n",
        "                # legomena tag distribution\n",
        "                position = emission_matrix_probabilities.columns.get_loc('unknown_token')\n",
        "            word_positions.append(position)\n",
        "        data = word_positions\n",
        "        # solve for sentence\n",
        "        path = viterbi_obj.Decode(data)\n",
        "        predicted_feature_list = []\n",
        "        # Transform in feature name\n",
        "        for state in path:\n",
        "            lexical_name = transition_matrix_probabilities.index[state]\n",
        "            predicted_feature_list.append(lexical_name)\n",
        "        # Check correct predicted\n",
        "        total_correct_predicted = 0\n",
        "        for feature, real_feature in zip(predicted_feature_list, feature_list):\n",
        "            # Check for each feature\n",
        "            if feature == real_feature:\n",
        "                if feature in feature_corrected_predicted:\n",
        "                    feature_corrected_predicted[feature] +=1\n",
        "                else: \n",
        "                    feature_corrected_predicted[feature] =1\n",
        "                # Total accuracy\n",
        "                total_correct_predicted+=1\n",
        "            else:\n",
        "                if real_feature not in feature_corrected_predicted:\n",
        "                    feature_corrected_predicted[real_feature] = 0\n",
        "\n",
        "        total_correct_predicted /= len(feature_list)\n",
        "        total_accuracy += total_correct_predicted\n",
        "        # Save feature frequency of this sentence\n",
        "        features_frequency = Counter(feature_list)\n",
        "        features_total_frequency = features_total_frequency + features_frequency\n",
        "\n",
        "    total_accuracy /= len(test_sentences)\n",
        "\n",
        "    for feature, frequency in features_total_frequency.items():\n",
        "        feature_corrected_predicted[feature] /= frequency\n",
        "\n",
        "    feature_accuracy = feature_corrected_predicted\n",
        "    return total_accuracy , feature_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tfFXllXW991",
        "colab_type": "text"
      },
      "source": [
        "## Using Crossvalidation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7Ljt2F_TGGy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kf = KFold(n_splits=10, shuffle=True, random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncUbVbogfrt6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('corpus100.txt', 'r') as f:\n",
        "    token_dict = {}\n",
        "    feature_list = []\n",
        "    file_line_list = []\n",
        "    transitions_count = {}\n",
        "    for line in f:\n",
        "        file_line_list.append(line)\n",
        "        \n",
        "sentences_array = np.array(file_line_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zyh54-07TNaJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "fe08f0c6-234b-4c1a-c9f8-4d8204ab3c44"
      },
      "source": [
        "total_folds_acc = 0\n",
        "total_feature_acc_list = []\n",
        "for train_idx, test_idx in kf.split(sentences_array):\n",
        "    train_sentences = sentences_array[train_idx]\n",
        "    test_sentences = sentences_array[test_idx]\n",
        "    emis_matrix_prob , trans_matrix_prob, start_prob = create_transition_and_emission_matrix(train_sentences)\n",
        "\n",
        "    start_prob = np.array([start_prob.to_numpy()]).T\n",
        "    trans_prob = trans_matrix_prob.to_numpy()\n",
        "    emis_prob = emis_matrix_prob.to_numpy()\n",
        "\n",
        "    d = Decoder(start_prob, trans_prob, emis_prob)\n",
        "\n",
        "    total_acc, feature_acc = accuracy_on_test(test_sentences, d, emis_matrix_prob, trans_matrix_prob)\n",
        "    \n",
        "    total_folds_acc += total_acc\n",
        "    total_feature_acc_list.append(feature_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 479/479 [00:01<00:00, 307.33it/s]\n",
            "100%|██████████| 479/479 [00:01<00:00, 313.55it/s]\n",
            "100%|██████████| 479/479 [00:01<00:00, 311.11it/s]\n",
            "100%|██████████| 479/479 [00:01<00:00, 316.13it/s]\n",
            "100%|██████████| 479/479 [00:01<00:00, 286.42it/s]\n",
            "100%|██████████| 479/479 [00:01<00:00, 319.05it/s]\n",
            "100%|██████████| 479/479 [00:01<00:00, 298.77it/s]\n",
            "100%|██████████| 479/479 [00:01<00:00, 318.15it/s]\n",
            "100%|██████████| 479/479 [00:01<00:00, 298.20it/s]\n",
            "100%|██████████| 479/479 [00:01<00:00, 311.83it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9POvMWOTztZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7ddb8a15-074f-4d3a-a2ef-ad6650edfbb4"
      },
      "source": [
        "total_folds_acc /= 10\n",
        "total_folds_acc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8745718975908963"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CR1Mt82Cc7Eq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_feature_acc = total_feature_acc_list[0]\n",
        "\n",
        "for feature_acc in total_feature_acc_list[1:]:\n",
        "    for feature in feature_acc:\n",
        "        if feature in total_feature_acc:\n",
        "            total_feature_acc[feature] += feature_acc[feature]\n",
        "        else:\n",
        "            total_feature_acc[feature] = feature_acc[feature]\n",
        "\n",
        "# 10 fold normalization\n",
        "for feature in total_feature_acc:\n",
        "    total_feature_acc[feature] /= 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eljacid-W4mL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = {}\n",
        "results['Total'] = total_folds_acc*100\n",
        "for feature in sorted(total_feature_acc):\n",
        "    percentage = total_feature_acc[feature]*100\n",
        "    results[feature] = percentage"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOnBbismOfhn",
        "colab_type": "text"
      },
      "source": [
        "## Convert tags to category and save results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYzvcDr4lev9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tag_category = {'Total': 'Total do sistema',\n",
        "'ADJ' : 'Adjetivo',\n",
        "'ADV' : 'Advérbio',\n",
        "'ART' : 'Artigo',\n",
        "'NUME' : 'Numeral',\n",
        "'N' : 'Substantivo comum',\n",
        "'NP' : 'Substantivo próprio',\n",
        "'CONJ' : 'Conjunção',\n",
        "'PRON' : 'Pronome',\n",
        "'PREP' : 'Preposição',\n",
        "'VERB' : 'Verbo',\n",
        "'I' : 'Interjeição',\n",
        "'LOCU' : 'Locução',\n",
        "'PDEN' : 'Palavra Denotativa',\n",
        "'PREP+ART' : 'Contração',\n",
        "'PREP+PREP' : 'Contração',\n",
        "'PREP+PD' : 'Contração',\n",
        "'PREP+PPR' : 'Contração',\n",
        "'PREP+PPOT' : 'Contração',\n",
        "'PREP+ADJ' : 'Contração',\n",
        "'PREP+N' : 'Contração',\n",
        "'PREP+PPOA' : 'Contração',\n",
        "'PREP+ADV' : 'Contração',\n",
        "'PPOA+PPOA' : 'Contração',\n",
        "'ADV+PPR' : 'Contração',\n",
        "'ADV+PPOA' : 'Contração',\n",
        "'ADJ+PPOA' : 'Contração',\n",
        "'RES' : 'Residuais',\n",
        "'.' : 'Pontuação',\n",
        "':' : 'Pontuação',\n",
        "';' : 'Pontuação',\n",
        "'-' : 'Pontuação',\n",
        "'(' : 'Pontuação',\n",
        "'!' : 'Pontuação',\n",
        "'?' : 'Pontuação',\n",
        "'...' : 'Pontuação',\n",
        "')' : 'Pontuação',\n",
        "'\"' : 'Pontuação',\n",
        "'{' : 'Pontuação',\n",
        "'}' : 'Pontuação',\n",
        "',' : 'Pontuação',\n",
        "'\\'' : 'Pontuação'}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iig3oWCndiSe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "category_result = {}\n",
        "for tag in results:\n",
        "    category = tag_category.get(tag, None)\n",
        "    if category is None:\n",
        "        category='Outras Tags'\n",
        "    if category in category_result:\n",
        "        category_result[category].append({tag:results[tag]})\n",
        "    else:\n",
        "        category_result[category]= [{tag:results[tag]}]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCewBFOgl2bL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result_file = open('result.txt','w')\n",
        "for category in category_result:\n",
        "    category_size = len(category_result[category])\n",
        "    category_acc = 0\n",
        "    result_file.write(f\"Taxa de acerto para a classe: {category}:\\n\")\n",
        "    for tag_acc in category_result[category]:\n",
        "        # pass\n",
        "        percentage = list(tag_acc.values())[0]\n",
        "        feature = list(tag_acc.keys())[0]\n",
        "        category_acc += percentage\n",
        "        if percentage > 0:\n",
        "            result_file.write(f\"Tag '{feature}': {percentage:.3f}%\\n\")\n",
        "        else:\n",
        "            result_file.write(f\"Tag '{feature}': Não presente nos corpus de teste. \\n\")\n",
        "    result_file.write(f\"Total da classe {category}: {category_acc/category_size:.3f}%\\n\\n\")\n",
        "result_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32YJTqqve5Rp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}