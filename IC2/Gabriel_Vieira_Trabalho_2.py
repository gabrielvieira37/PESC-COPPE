# -*- coding: utf-8 -*-
"""Homework2_IC2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dMczg6QUW2aif-p-0xmoHTXd4gzkvW9p
"""

!wget https://sci2s.ugr.es/keel/dataset/data/classification/banana.zip
!unzip banana.zip

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from sklearn.metrics import accuracy_score, zero_one_loss
from sklearn import neighbors

data = np.loadtxt('banana.dat', comments='@', delimiter=',')
banana_df = pd.DataFrame(data, columns=['x','y','label'])

plt.figure(figsize=(12,9))
plt.title('Banana data points')
plt.ylabel('y')
plt.xlabel('x')
positive_banana = banana_df[banana_df['label'] == 1]
negative_banana = banana_df[banana_df['label'] == -1]
plt.scatter(negative_banana['x'], negative_banana['y'], c='r', alpha=0.7, label='negative')
plt.scatter(positive_banana['x'], positive_banana['y'], c='b', alpha=0.7, label='positive')
plt.legend()
plt.show()

print(f"There are {len(positive_banana)} positives examples")
print(f"There are {len(negative_banana)} negative examples")

"""## SVM experiment"""

from sklearn.model_selection import KFold
from sklearn import svm
from tqdm import tqdm
X = banana_df[['x','y']]
y = banana_df['label']

def plot_svc_decision_function(model, ax=None, plot_support=True):
    """Plot the decision function for a 2D SVC"""
    if ax is None:
        ax = plt.gca()
    xlim = ax.get_xlim()
    ylim = ax.get_ylim()
    
    # create grid to evaluate model
    x = np.linspace(xlim[0], xlim[1], 30)
    y = np.linspace(ylim[0], ylim[1], 30)
    Y, X = np.meshgrid(y, x)
    xy = np.vstack([X.ravel(), Y.ravel()]).T
    P = model.decision_function(xy).reshape(X.shape)
    
    # plot decision boundary and margins
    ax.contour(X, Y, P, colors='k',
               levels=[-1, 0, 1], alpha=0.5,
               linestyles=['--', '-', '--'])
    
    # plot support vectors
    if plot_support:
        ax.scatter(model.support_vectors_[:, 0],
                   model.support_vectors_[:, 1],
                   s=300, linewidth=1, facecolors='none');
    ax.set_xlim(xlim)
    ax.set_ylim(ylim)

models_list = ['sigmoid','rbf','linear','poly']
accuracy_matrix = {}
loss_matrix = {}

sigmoid_params = [1.0, 0.5, 0.01]
fold_sizes = [2, 5, 10]

accuracy_df = pd.DataFrame()
loss_df = pd.DataFrame()
models_df = pd.DataFrame()
# f_size = 2

for f_size in tqdm(fold_sizes):
    kf = KFold(n_splits=f_size, shuffle=True, random_state=123)
    fold_model_total_acc = None
    fold_model_total_loss = None
    svm_models_objects = {}
    for train_idx, test_idx in kf.split(X,y):
        X_train = X[X.index.isin(train_idx)]
        y_train = y[y.index.isin(train_idx)]

        X_test = X[X.index.isin(test_idx)]
        y_test = y[y.index.isin(test_idx)]

        model_accuracy = {}
        model_loss = {}
        for model in models_list:    
            if model=='sigmoid':
                # Run for each sigmoid parameter
                for param in sigmoid_params:
                    svm_model = svm.SVC(kernel=model, coef0=param)
                    svm_model.fit(X_train, y_train)
                    y_pred = svm_model.predict(X_test)
                    model_accuracy[model+'-'+str(param)] = accuracy_score(y_pred, y_test)
                    model_loss[model+'-'+str(param)] = zero_one_loss(y_test,y_pred)
                    svm_models_objects[model+'-'+str(param)] = svm_model
            else:
                svm_model = svm.SVC(kernel=model)
                svm_model.fit(X_train, y_train)
                y_pred = svm_model.predict(X_test)
                model_accuracy[model] = accuracy_score(y_pred, y_test)
                model_loss[model] = zero_one_loss(y_test,y_pred)
                svm_models_objects[model] = svm_model
        
        if fold_model_total_acc is None:
            fold_model_total_acc = pd.Series(model_accuracy)
        else:
            fold_model_total_acc += pd.Series(model_accuracy)

        if fold_model_total_loss is None:
            fold_model_total_loss = pd.Series(model_loss)
        else:
            fold_model_total_loss += pd.Series(model_loss)

    models_series = pd.Series(svm_models_objects)

    fold_model_total_acc/=f_size
    fold_model_total_loss/=f_size

    accuracy_df[f'{f_size}_folds'] = fold_model_total_acc
    loss_df[f'{f_size}_folds'] = fold_model_total_loss
    models_df[f'{f_size}_folds']= models_series

accuracy_df

loss_df

models_df

"""### Worst model"""

svm_model = models_df['2_folds']['sigmoid-0.01']

x_plot_support_vec = svm_model.support_vectors_[:,0]
y_plot_support_vec = svm_model.support_vectors_[:,1]

plt.figure(figsize=(12,9))
plt.title('Banana data points')
plt.ylabel('y')
plt.xlabel('x')
positive_banana = banana_df[banana_df['label'] == 1]
negative_banana = banana_df[banana_df['label'] == -1]
plt.scatter(negative_banana['x'], negative_banana['y'], c='r', alpha=0.7, label='negative')
plt.scatter(positive_banana['x'], positive_banana['y'], c='b', alpha=0.7, label='positive')
plt.scatter(x_plot_support_vec, y_plot_support_vec, c='g', alpha=0.3, label='support')
plt.legend()
plot_svc_decision_function(svm_model)
plt.show()

"""### Best Model"""

svm_model = models_df['10_folds']['rbf']

x_plot_support_vec = svm_model.support_vectors_[:,0]
y_plot_support_vec = svm_model.support_vectors_[:,1]

plt.figure(figsize=(12,9))
plt.title('Banana data points')
plt.ylabel('y')
plt.xlabel('x')
positive_banana = banana_df[banana_df['label'] == 1]
negative_banana = banana_df[banana_df['label'] == -1]
plt.scatter(negative_banana['x'], negative_banana['y'], c='r', alpha=0.7, label='negative')
plt.scatter(positive_banana['x'], positive_banana['y'], c='b', alpha=0.7, label='positive')
plt.scatter(x_plot_support_vec, y_plot_support_vec, c='g', alpha=0.3, label='support')
plt.legend()
plot_svc_decision_function(svm_model)
plt.show()

"""### KNN"""

n_fold = 10
kf = KFold(n_splits=n_fold, shuffle=True, random_state=123)
avg_acc = 0
avg_loss = 0
for train_idx, test_idx in kf.split(X,y):
        knn = neighbors.KNeighborsClassifier(weights='distance', metric='euclidean')
        X_train = X[X.index.isin(train_idx)]
        y_train = y[y.index.isin(train_idx)]

        X_test = X[X.index.isin(test_idx)]
        y_test = y[y.index.isin(test_idx)]
        
        knn.fit(X_train, y_train)
        y_pred = knn.predict(X_test)
        avg_acc += accuracy_score(y_pred, y_test)
        avg_loss += zero_one_loss(y_test,y_pred)
avg_acc /= n_fold
avg_loss /= n_fold

print(f"Average accuracy on knn: {avg_acc}")
print(f"Average out of sample error on knn: {avg_loss}")

knn = neighbors.KNeighborsClassifier(weights='distance', metric='euclidean')
knn.fit(X, y)

h = .2
# Create color maps
cmap_light = ListedColormap(['orange', 'cyan', 'cornflowerblue'])
cmap_bold = ListedColormap(['darkorange', 'c', 'darkblue'])
n_neighbors = 5
weights = 'distance'

x_min, x_max = X['x'].min() - 1, X['x'].max() + 1
y_min, y_max = X['y'].min() - 1, X['y'].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                        np.arange(y_min, y_max, h))
Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])

# Put the result into a color plot
Z = Z.reshape(xx.shape)
plt.figure(figsize=(12,9))
plt.pcolormesh(xx, yy, Z, cmap=cmap_light)

# Plot also the training points
plt.scatter(X['x'], X['y'], c=y, cmap=cmap_bold,
            edgecolor='k', s=20)
plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.xlabel('x')
plt.ylabel('y')
plt.title("Banana dataset classification")

plt.show()

"""## CNN Experiment"""

from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten
from tensorflow.keras.utils import to_categorical
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tqdm import tqdm
from sklearn.model_selection import KFold

mnist = keras.datasets.mnist

def define_model():
    num_filters = 8
    filter_size = 3
    pool_size = 2

    model = Sequential([
    Conv2D(num_filters, filter_size, input_shape=(28, 28, 1), kernel_initializer='he_uniform', activation='relu'),
    MaxPooling2D(pool_size=pool_size),
    Flatten(),
    Dense(10, activation='softmax'),
    ])

    model.compile(
    'adam',
    loss='categorical_crossentropy',
    metrics=['accuracy'],
    )

    return model

def define_model_double():
    num_filters = 8
    filter_size = 3
    pool_size = 2

    model = Sequential([
    Conv2D(num_filters, filter_size, input_shape=(28, 28, 1), kernel_initializer='he_uniform', activation='relu'),
    MaxPooling2D(pool_size=pool_size),
    Conv2D(num_filters**2, filter_size, input_shape=(28, 28, 1), kernel_initializer='he_uniform', activation='relu'),
    MaxPooling2D(pool_size=pool_size),
    Flatten(),
    Dense(10, activation='softmax'),
    ])

    model.compile(
    'adam',
    loss='categorical_crossentropy',
    metrics=['accuracy'],
    )

    return model

def check_model_performance(model, X, y):
    history_first_moment = None
    history_second_moment = None
    n_fold = 10
    kf = KFold(n_splits=n_fold, shuffle=True, random_state=123)
    for train_idx, val_idx in tqdm(kf.split(X, y)):
        X_train = X[train_idx]
        y_train = y[train_idx]

        X_val = X[val_idx]
        y_val = y[val_idx]
        
        history = model.fit(X_train, to_categorical(y_train), epochs=5, validation_data=(X_val, to_categorical(y_val)))
        history = history.history
        history_df = pd.DataFrame(history)
        if history_first_moment is None:
            history_first_moment = history_df
            history_second_moment = history_df**2
        else:
            history_first_moment += history_df
            history_second_moment += history_df**2


    history_std = (history_second_moment - (history_first_moment.copy()**2)/n_fold)/(n_fold-1)
    history_avg = history_first_moment.copy()/n_fold

    return history_avg, history_std, X_train, X_val

# 28x28 MNIST images 
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

train_images.shape

# Normalize to 0-1
train_images = (train_images / 255)
test_images = (test_images / 255)

# Reshape the images.
train_images = np.expand_dims(train_images, axis=3)
test_images = np.expand_dims(test_images, axis=3)

print(train_images.shape) # (60000, 28, 28, 1)
print(test_images.shape)  # (10000, 28, 28, 1)

all_images = np.concatenate((train_images, test_images))
all_labels = np.concatenate((train_labels, test_labels))

print(all_images.shape)
print(all_labels.shape)
X = all_images
y = all_labels

"""### MNIST dataset"""

fig, ax = plt.subplots(2,5 , figsize=(12,9))
fig.suptitle('Mnist sample')
for i, ax in enumerate(ax.flatten()):
    im_idx = np.argwhere(y == i)[0]
    plottable_image = np.reshape(X[im_idx], (28, 28))
    ax.imshow(plottable_image, cmap='gray_r')
    ax.set_title(f'Class: {i}')

model = define_model()

history_avg, history_std, X_train, X_val = check_model_performance(model, X, y)

# putting index from 1 to 5
history_avg.index = history_avg.index+1
history_std.index = history_std.index+1

"""### Training performance"""

fig, (ax1, ax2) = plt.subplots(2, 1 , figsize=(12,12))

ax1.plot(history_avg['accuracy'], 'r', marker='o', alpha=0.7)
ax1.set_title('Model accuracy')
ax1.errorbar(x=history_avg.index,y=history_avg['accuracy'],yerr=history_std['accuracy'], linestyle=None, fmt=',k', label='std')
ax1.set_ylabel('Accuracy')
ax1.set_xlabel('epoch')
ax1.legend(['mean', 'std'], loc='upper left')
ax1.grid()

ax2.plot(history_avg['loss'], 'r', marker='o', alpha=0.7)
ax2.set_title('Model Loss')
ax2.errorbar(x=history_avg.index,y=history_avg['loss'],yerr=history_std['loss'], linestyle=None, fmt=',k', label='std')
ax2.set_ylabel('Loss')
ax2.set_xlabel('epoch')
ax2.legend(['mean', 'std'], loc='upper right')
ax2.grid()
plt.show()

val_acc = history_avg['val_accuracy'].iloc[-1]
val_loss = history_avg['val_loss'].iloc[-1]
print(f"Final validation accuracy: {val_acc}, final validation loss: {val_loss}")

model.summary()

"""### CNN explained with SHAP"""

!pip install shap

import shap
import os
import numpy as np
from sklearn.model_selection import train_test_split
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

# select a set of background examples to take an expectation over
background = X_train[np.random.choice(X_train.shape[0], 100, replace=False)]

# explain predictions of the model on five images
e = shap.DeepExplainer(model, background)
# ...or pass tensors directly
# e = shap.DeepExplainer((model.layers[0].input, model.layers[-1].output), background)
shap_values = e.shap_values(X_val[20:25])

# plot the feature attributions
shap.image_plot(shap_values, -X_val[20:25])

"""### Trying new CNN architecture"""

model_double = define_model_double()

model_double.summary()

history_avg, history_std, X_train, X_val = check_model_performance(model_double, X, y)

val_acc = history_avg['val_accuracy'].iloc[-1]
val_loss = history_avg['val_loss'].iloc[-1]
print(f"Final validation accuracy: {val_acc}, final validation loss: {val_loss}")

# select a set of background examples to take an expectation over
background = X_train[np.random.choice(X_train.shape[0], 100, replace=False)]

# explain predictions of the model on five images
e = shap.DeepExplainer(model_double, background)
# ...or pass tensors directly
# e = shap.DeepExplainer((model.layers[0].input, model.layers[-1].output), background)
shap_values = e.shap_values(X_val[20:25])

# plot the feature attributions
shap.image_plot(shap_values, -X_val[20:25])

